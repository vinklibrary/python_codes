{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layer with one unit. A sigmoid activation makes it a logistic regression (binary linear classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUTS = 10\n",
    "logistic_regression = nn.Sequential(\n",
    "    nn.Linear(NUM_INPUTS,1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layer with one unit. No activation makes it a linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUTS = 10\n",
    "linear_regression = nn.Sequential(\n",
    "    nn.Linear(NUM_INPUTS,1),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layer with many units. Softmax activation makes it a “softmax classifier”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUTS = 10\n",
    "NUM_OUTPUTS = 5\n",
    "softmax_classifier = nn.Sequential(\n",
    "    nn.Linear(NUM_INPUTS,NUM_OUTPUTS),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many fully connected layers with many units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUTS = 100\n",
    "HIDDEN_SIZES = 1024\n",
    "NUM_OUTPUTS = 20\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(NUM_INPUTS, HIDDEN_SIZES),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(HIDDEN_SIZES, HIDDEN_SIZES),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(HIDDEN_SIZES, NUM_OUTPUTS),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually take one-hot codes as discrete tokens. Can we use a Linear layer to process it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x tensor size:  torch.Size([10, 10000])\n",
      "Input y tensor size:  torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_SIZE = 100\n",
    "#mapping a vocabulary of size 10,000 to HIDDEN_SIZE projections\n",
    "emb_1 = nn.Linear(VOCAB_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "# forward example [10, 10000] tensor\n",
    "code = [1] + [0]*9999\n",
    "# copy 10 times the same code [1 0 0 0 ... 0]\n",
    "x = torch.Tensor([code] * 10)\n",
    "print(\"Input x tensor size: \", x.size())\n",
    "y = emb_1(x)\n",
    "print(\"Input y tensor size: \", y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x tensor size:  torch.Size([10, 1])\n",
      "Input y tensor size:  torch.Size([10, 1, 100])\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_SIZE = 100\n",
    "#mapping a vocabulary of size 10,000 to HIDDEN_SIZE projections\n",
    "emb_2 = nn.Embedding(VOCAB_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "x = torch.zeros(10, 1).long()\n",
    "print('Input x tensor size: ' , x.size())\n",
    "y = emb_2(x)\n",
    "print('Input y tensor size: ' , y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def one_hot_embedding(labels, num_classes):\n",
    "    '''Embedding labels to one-hot.\n",
    "\n",
    "    Args:\n",
    "      labels: (LongTensor) class labels, sized [N,].\n",
    "      num_classes: (int) number of classes.\n",
    "\n",
    "    Returns:\n",
    "      (tensor) encoded labels, sized [N,#classes].\n",
    "    '''\n",
    "    y = torch.eye(num_classes, device='cpu')  # [D,D]\n",
    "    return y[labels]  # [N,D]\n",
    "    '''\n",
    "    创建   num_classes维度的单位矩阵\n",
    "    然后取出单位矩阵的某一行，即可以作为 one-hot vector\n",
    "    '''\n",
    "\n",
    "labels=2\n",
    "num_classes=10\n",
    "one_hot_vector=one_hot_embedding(labels,num_classes)\n",
    "print(one_hot_vector)\n",
    "#tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
